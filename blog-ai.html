<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>All You Need to Know to Make Conversation with an AI Scientist - Blog Post</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: #f4f1de;
            color: #3d405b;
            min-height: 100vh;
            line-height: 1.6;
        }
        nav {
            background: #3d405b;
            padding: 1rem;
            margin-bottom: 2rem;
        }
        nav ul {
            display: flex;
            justify-content: center;
            gap: 2rem;
            list-style: none;
        }
        nav a {
            color: #f4f1de;
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        nav a:hover {
            color: #81b29a;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 2rem;
        }
        .blog-post {
            background: #fff;
            border-radius: 15px;
            padding: 3rem;
            margin-bottom: 2rem;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }
        .blog-post h1 {
            color: #e07a5f;
            margin-bottom: 1rem;
            font-size: 2.5rem;
        }
        .blog-post h2 {
            color: #e07a5f;
            margin-top: 2rem;
            margin-bottom: 1rem;
            font-size: 1.8rem;
        }
        .post-meta {
            color: #81b29a;
            font-size: 0.9rem;
            margin-bottom: 2rem;
            padding-bottom: 1rem;
            border-bottom: 1px solid #eee;
        }
        .post-content {
            font-size: 1.1rem;
            line-height: 1.8;
            color: #3d405b;
        }
        .post-content p {
            margin-bottom: 1.5rem;
        }
        .back-link {
            display: inline-block;
            margin-bottom: 2rem;
            color: #81b29a;
            text-decoration: none;
            transition: color 0.3s ease;
        }
        .back-link:hover {
            color: #3d405b;
        }
        .equation {
            text-align: center;
            margin: 1.5rem 0;
            font-size: 1.2rem;
        }
        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }
            
            .blog-post {
                padding: 1.5rem;
            }
        }
    </style>
    <!-- Add MathJax script -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</head>
<body>
    <nav>
        <ul>
            <li><a href="index.html">Home</a></li>
            <li><a href="tinkering.html">Tinkering</a></li>
            <li><a href="blog.html">Blog</a></li>
        </ul>
    </nav>
    <div class="container">
        <a href="blog.html" class="back-link">← Back to blog</a>
        
        <article class="blog-post">
            <h1>All You Need to Know to Make Conversation with an AI Scientist</h1>
            <div class="post-meta">February 21, 2025</div>
            <div class="post-content">
                <p>AI this, AI that — we have been hearing a lot of AI happening, but what is the core mathematics and rationale that runs and powers this boom? In this post, I will be covering some basics of what makes AI tick and discuss the basic and fundamental architectures that are the building blocks of this AI boom.</p>
                
                <p>Alright, on to the main topic. This post presumes some basics in concepts like probability and basic mathematical intuition.</p>
                
                <p>What machine learning and Artificial Intelligence tries to do is to write some sort of mathematical equation and then estimate or predict an output for corresponding input parameters. Bluntly, it is a very large and modified prediction problem, that's it!</p>
                
                <h2>1. The Basics: Linear Regression (A Fancy Best Fit Line)</h2>
                
                <p>If you have been to a physics lab in your 12th grade or college, you will have made a best fit line for some experiment. Congratulations, you have already implemented Linear Regression!</p>
                
                <p>A linear regression presumes that your target is a linear function of the input parameters with the equation looking like:</p>
                
                <div class="equation">\[ y = a_1 x_1 + a_2 x_2 + \dots + a_n x_n + b \]</div>
                
                <p>A fancy way to write this would be using matrices as:</p>
                
                <div class="equation">\[ y = W^T X + b \]</div>
                
                <p>where W is a weight vector [a₁, a₂, ..., aₙ]<sup>T</sup> and X is the input feature vector [x₁, x₂, ..., xₙ]<sup>T</sup>. This is the most simple modeling approach.</p>
                
                <h2>2. The Classifiers: Logistic Regression, Naive Bayes, SVMs</h2>
                
                <h3 style="color: #81b29a; margin-top: 1rem; margin-bottom: 0.8rem; font-size: 1.4rem;">• Logistic Regression</h3>
                <p>Consider the diagram below for the sigmoid function:</p>
                
                <div class="equation">\[ y = \frac{1}{1 + e^{-z}} \]</div>
                
                <p>You can see that for extreme values of z, y goes to 0 or 1. <br>

                    <img src="sigmoid.png" alt="Sigmoid Function" style="display: block; margin: 1.5rem auto; max-width: 100%; height: auto;"></p>
                   <p> Congratulations! You have now created a continuous differentiable function (yes that is of importance, especially in backpropagation in complex systems) which acts like an "IF-ELSE"! 
                    <p>In general, one parameter is not enough, so we do a linear regression on z, thus making the general equation as:</p>
                
                <div class="equation">\[ y = \frac{1}{1 + e^{-(W^T X + b)}} \]</div>
                
                <h3 style="color: #81b29a; margin-top: 1rem; margin-bottom: 0.8rem; font-size: 1.4rem;">• Naive Bayes</h3>
                <p>Unlike other algorithms, Naive Bayes presumes all events to be statistically probabilistic and can be modelled using Bayesian probability (i.e., follows Bayes' rule).</p>
                
                <div class="equation">\[ P(A \mid B) = \frac{P(B \mid A)\times P(A)}{P(B)} \]</div>
                
                <h3 style="color: #81b29a; margin-top: 1rem; margin-bottom: 0.8rem; font-size: 1.4rem;">• SVM</h3>
                <!-- <p>Support Vector Machines (SVMs) find optimal hyperplanes that separate different classes.</p> -->
                <p>But sometimes, a plain old line isn't enough to capture reality — for instance, projectile motion often looks like:</p>
                <div class="equation">\[ s = a\cdot t + b\cdot t^2 \]</div>
                <p>which clearly won't fit a straight line. In these cases, SVMs can tap into hyperplanes (multinomials) to handle these curved, multi-dimensional relationships. 
                    <!-- <a href="projectile_motion.png" target="_blank">Check out this quick projectile example!</a> -->
                </p>
                <p>Polynomial kernels let SVMs capture curved boundaries. Formally, a polynomial multi-dimensional SVM looks like:</p>
                <div class="equation">\[ K(x, x') = (x^T x' + C)^d  \]</div>
                <p>where \(x\) and \(x'\) are your feature vectors, \(x^T x'\) is their dot product, and \(d\) is the kernel degree (i.e., how "polynomial" you want the relationship to be).</p>

                <!-- <p>We can make this more sophisticated by adding a constant \(c\) before exponentiation:</p>
                <div class="equation">\[ K(\mathbf{x}, \mathbf{x}') = (\mathbf{x}^T \mathbf{x}' + c)^d \]</div>
                <p>Here, \(c\) shifts the feature space (often improving performance when data isn't centered), and raising it to the power of \(d\) allows for more complex decision boundaries. Higher \(d\) can model more intricate structures but might lead to overfitting if \(d\) is too large.</p> -->
                
                <p>For the math enthusiasts, the polynomial kernel can be expanded as:</p>
                <div class="equation">

\[ (x^T x')^d = \sum_{i_1 + i_2 + \dots + i_n = d} \frac{d!}{i_1! \, i_2! \, \dots \, i_n!} \,
x_1^{i_1} x_2^{i_2} \dots x_n^{i_n} \,\,(x_1')^{i_1} (x_2')^{i_2} \dots (x_n')^{i_n} \]
</div>
                <p>Here, \(i_1, i_2, \dots, i_n\) are non-negative integers, and the sum of their powers equals \(d\). The multinomial coefficient \(\frac{d!}{i_1! \, i_2! \, \dots \, i_n!}\) counts the number of ways to distribute \(d\) items into \(n\) bins with \(i_1\) items in the first bin, \(i_2\) in the second, and so on.</p>
                <h2>3. Onset of Complexity: Neural Networks</h2>
                
                <p>So far we have been modelled quantities as multivariate polynomials, but that may not be enough for most prediction tasks. For slightly more complex tasks, such as predicting if an image is a dog or not?</p>

                <p>What we try to do is make multiple predictions, on top of other predictions.</p>

                <p>We fix the number of parameters, model some inherent feature, then try to predict upon the presence/score of THAT feature and so on.</p>

                <p>Something like:</p>

                <div class="equation">\[
                y = f_{L}\Bigl(
                W_{L} \cdot f_{L-1}\bigl(
                W_{L-1} \cdot \dots f_{1}(W_{1}\cdot x + b_{1}) \dots
                + b_{L-1}
                \bigr)
                + b_{L}
                \Bigr)
                \]</div>

                <p>Where:<br>
                x is the input vector<br>
                y is the output vector<br>
                L is the number of layers<br>
                W<sub>i</sub> are the weight matrices for each layer<br>
                b<sub>i</sub> are the bias vectors for each layer<br>
                f<sub>i</sub> are the activation functions for each layer
                </p>
                
                <p>Here's a visual explanation to help you understand better:</p>
                <div style="text-align: center;">
                    <img src="neural.jpg" alt="Neural Network Diagram" style="max-width: 100%; height: auto; margin: 1.5rem 0;">
                    <p style="font-size: 0.9rem; color: #3d405b;">Figure: A simple neural network with input, hidden, and output layers.</p>
                </div>

                <h2>4. Sequential NNs: RNNs (and more)</h2>
                <p>Congratulations on making it this far! Until now, we've discussed models handling inputs without explicitly capturing dependencies across time or sequence. With RNNs (and similar architectures), we introduce mechanisms to retain and utilize historical information, unlocking a new category of inferences.</p>
                <p>A common representation of a simple RNN is given by:</p>
                <div class="equation">
\[ h_t = \tanh\bigl(W_x\,x_t + W_h\,h_{t-1} + b_h\bigr) \]
</div>
<p>where:</p>
<ul>
  <li>\(x_t\) is the input at time step \(t\).</li>
  <li>\(h_t\) is the hidden state at time step \(t\).</li>
  <li>\(W_x\) is the weight matrix from input to hidden.</li>
  <li>\(W_h\) is the weight matrix from the previous hidden state to the current hidden state.</li>
  <li>\(b_h\) is the bias term for the hidden state.</li>
</ul>
<br>
<p>The hidden state \(h_t\) is passed to the next time step as \(h_{t-1}\), enabling the network to capture sequential dependencies by carrying information forward in time. This transfer of the hidden state is what allows RNNs to “remember” relevant aspects of the input history and apply those insights to future predictions.</p>
                
<p>Since only binary information is transfered from each state to next, and not all information is relevant hence this and similar architectures(LSTMs, GRUs) fail for very large sequences.</p>
<h2>5. Daddy of ChatGPT: The Transformer</h2>
                
                <p>Transformers revolutionized NLP with their attention mechanism, which weighs the importance of different parts of the input sequence. Unlike RNNs, transformers process entire sequences in parallel, making them more efficient. The self-attention mechanism allows each position in the sequence to attend to all positions, capturing long-range dependencies effectively. This architecture powers modern large language models like GPT (Generative Pre-trained Transformer), which produces human-like text by predicting the next token in a sequence based on previous context.</p>
            </div>
        </article>
    </div>
</body>
</html>